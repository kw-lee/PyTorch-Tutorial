{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Logistic Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Seungjae Lee (이승재)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "    We use elemental PyTorch to implement linear regression here. However, in most actual applications, abstractions such as <code>nn.Module</code> or <code>nn.Linear</code> are used. You can see those implementations near the end of this notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(X) = \\frac{1}{1+e^{-W^T X}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - If $y \\simeq H(x)$, cost is near 0.\n",
    " - If $y \\neq H(x)$, cost is high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight Update via Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W := W - \\alpha \\frac{\\partial}{\\partial W} cost(W) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - $\\alpha$: Learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x110b3a670>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For reproducibility\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following classification problem: given the number of hours each student spent watching the lecture and working in the code lab, predict whether the student passed or failed a course. For example, the first (index 0) student watched the lecture for 1 hour and spent 2 hours in the lab session ([1, 2]), and ended up failing the course ([0])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(np.array(x_data)[0:3,0], np.array(x_data)[0:3,1], 'bx')\n",
    "plt.plot(np.array(x_data)[3:,0], np.array(x_data)[3:,1], 'rx')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, we need these data to be in `torch.Tensor` format, so we convert them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(X) = \\frac{1}{1+e^{-W^T X}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch has a `torch.exp()` function that resembles the exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e^1 equals:  tensor([2.7183])\n"
     ]
    }
   ],
   "source": [
    "print('e^1 equals: ', torch.exp(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use it to compute the hypothesis function conveniently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<MulBackward0>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, we could use `torch.sigmoid()` function! This resembles the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/(1+e^{-1}) equals:  tensor([0.7311])\n"
     ]
    }
   ],
   "source": [
    "print('1/(1+e^{-1}) equals: ', torch.sigmoid(torch.FloatTensor([1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the code for hypothesis function is cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "torch.Size([6, 1])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(hypothesis.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Cost Function (Low-level)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to measure the difference between `hypothesis` and `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]], grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]])\n"
     ]
    }
   ],
   "source": [
    "print(hypothesis)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one element, the loss can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6931], grad_fn=<NegBackward>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-(y_train[0] * torch.log(hypothesis[0]) + \n",
    "  (1 - y_train[0]) * torch.log(1 - hypothesis[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the losses for the entire batch, we can simply input the entire vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931],\n",
      "        [0.6931]], grad_fn=<NegBackward>)\n"
     ]
    }
   ],
   "source": [
    "losses = -(y_train * torch.log(hypothesis) + \n",
    "           (1 - y_train) * torch.log(1 - hypothesis))\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we just `.mean()` to take the mean of these individual losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "cost = losses.mean()\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Cost Function with `F.binary_cross_entropy`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, binary classification is used so often that PyTorch has a simple function called `F.binary_cross_entropy` implemented to lighten the burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.binary_cross_entropy(hypothesis, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Low-level Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n",
    "y_data = [[0], [0], [0], [1], [1], [1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031673\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
    "    cost = -(y_train * torch.log(hypothesis) + \n",
    "             (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with `F.binary_cross_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000 Cost: 0.693147\n",
      "Epoch  100/1000 Cost: 0.134722\n",
      "Epoch  200/1000 Cost: 0.080643\n",
      "Epoch  300/1000 Cost: 0.057900\n",
      "Epoch  400/1000 Cost: 0.045300\n",
      "Epoch  500/1000 Cost: 0.037261\n",
      "Epoch  600/1000 Cost: 0.031672\n",
      "Epoch  700/1000 Cost: 0.027556\n",
      "Epoch  800/1000 Cost: 0.024394\n",
      "Epoch  900/1000 Cost: 0.021888\n",
      "Epoch 1000/1000 Cost: 0.019852\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((2, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = xy[:, 0:-1]\n",
    "y_data = xy[:, [-1]]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.FloatTensor(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n",
      "        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n",
      "        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n",
      "        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n",
      "        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0:5])\n",
    "print(y_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Real Data using low-level Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.693148\n",
      "Epoch   10/100 Cost: 0.572727\n",
      "Epoch   20/100 Cost: 0.539493\n",
      "Epoch   30/100 Cost: 0.519708\n",
      "Epoch   40/100 Cost: 0.507066\n",
      "Epoch   50/100 Cost: 0.498539\n",
      "Epoch   60/100 Cost: 0.492549\n",
      "Epoch   70/100 Cost: 0.488208\n",
      "Epoch   80/100 Cost: 0.484985\n",
      "Epoch   90/100 Cost: 0.482543\n",
      "Epoch  100/100 Cost: 0.480661\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((8, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
    "    cost = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis)).mean()\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 10번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with Real Data using `F.binary_cross_entropy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.693147\n",
      "Epoch   10/100 Cost: 0.572727\n",
      "Epoch   20/100 Cost: 0.539494\n",
      "Epoch   30/100 Cost: 0.519708\n",
      "Epoch   40/100 Cost: 0.507065\n",
      "Epoch   50/100 Cost: 0.498539\n",
      "Epoch   60/100 Cost: 0.492549\n",
      "Epoch   70/100 Cost: 0.488208\n",
      "Epoch   80/100 Cost: 0.484985\n",
      "Epoch   90/100 Cost: 0.482543\n",
      "Epoch  100/100 Cost: 0.480661\n"
     ]
    }
   ],
   "source": [
    "# 모델 초기화\n",
    "W = torch.zeros((8, 1), requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "# optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # Cost 계산\n",
    "    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # 10번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "            epoch, nb_epochs, cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the Accuracy our our Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we finish training the model, we want to check how well our model fits the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4103],\n",
      "        [0.9242],\n",
      "        [0.2300],\n",
      "        [0.9411],\n",
      "        [0.1772]], grad_fn=<SliceBackward>)\n"
     ]
    }
   ],
   "source": [
    "hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n",
    "print(hypothesis[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can change **hypothesis** (real number from 0 to 1) to **binary predictions** (either 0 or 1) by comparing them to 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False]])\n"
     ]
    }
   ],
   "source": [
    "prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "print(prediction[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compare it with the correct labels `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False],\n",
      "        [ True],\n",
      "        [False],\n",
      "        [ True],\n",
      "        [False]])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "print(prediction[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True],\n",
      "        [True]])\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = prediction.float() == y_train\n",
    "print(correct_prediction[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can calculate the accuracy by counting the number of correct predictions and dividng by total number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has an accuracy of 76.68% for the training set.\n"
     ]
    }
   ],
   "source": [
    "accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "print('The model has an accuracy of {:2.2f}% for the training set.'.format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: High-level Implementation with `nn.Module`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(8, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigmoid(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 Cost: 0.704829 Accuracy 45.72%\n",
      "Epoch   10/100 Cost: 0.572391 Accuracy 67.59%\n",
      "Epoch   20/100 Cost: 0.539563 Accuracy 73.25%\n",
      "Epoch   30/100 Cost: 0.520042 Accuracy 75.89%\n",
      "Epoch   40/100 Cost: 0.507561 Accuracy 76.15%\n",
      "Epoch   50/100 Cost: 0.499125 Accuracy 76.42%\n",
      "Epoch   60/100 Cost: 0.493177 Accuracy 77.21%\n",
      "Epoch   70/100 Cost: 0.488846 Accuracy 76.81%\n",
      "Epoch   80/100 Cost: 0.485612 Accuracy 76.28%\n",
      "Epoch   90/100 Cost: 0.483146 Accuracy 76.55%\n",
      "Epoch  100/100 Cost: 0.481234 Accuracy 76.81%\n"
     ]
    }
   ],
   "source": [
    "# optimizer 설정\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    hypothesis = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.binary_cross_entropy(hypothesis, y_train)\n",
    "\n",
    "    # cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # 20번마다 로그 출력\n",
    "    if epoch % 10 == 0:\n",
    "        prediction = hypothesis >= torch.FloatTensor([0.5])\n",
    "        correct_prediction = prediction.float() == y_train\n",
    "        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n",
    "        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n",
    "            epoch, nb_epochs, cost.item(), accuracy * 100,\n",
    "        ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
